{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NER",
   "id": "7bc0719ea48a069f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T17:49:07.073697Z",
     "start_time": "2024-12-30T17:49:05.467120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "def extract_named_entities(abstract_text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        abstract_text (str): The text of the abstract.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary represents a named entity\n",
    "              and contains the 'text' of the entity and its 'label'.\n",
    "    \"\"\"\n",
    "    # Load the English language model from spaCy\n",
    "    # You might need to download it first if you haven't already:\n",
    "    # python -m spacy download en_core_web_sm\n",
    "    #nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    doc = nlp(abstract_text)\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "    # Extract named entities\n",
    "    named_entities = []\n",
    "    for ent in doc.ents:\n",
    "        named_entities.append({\"text\": ent.text, \"label\": ent.label_})\n",
    "\n",
    "    # named_entities = []\n",
    "    # for ent in doc.ents:\n",
    "    #     if ent.label_ in [\"ORG\", \"PERSON\"]:  # Extract only organizations and people\n",
    "    #         named_entities.append({\"text\": ent.text, \"label\": ent.label_})\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "abstract = \"\"\"\n",
    "The study investigates the efficacy of a novel drug, Aliprex, in treating patients with Alzheimer's disease. \n",
    "The research was conducted at the University of California, San Francisco (UCSF) and involved 100 participants. \n",
    "Preliminary results indicate a significant improvement in cognitive function among patients receiving Aliprex compared to the placebo group. \n",
    "The research team, led by Dr. Emily Carter, plans to publish the full findings in the New England Journal of Medicine.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_named_entities(abstract)"
   ],
   "id": "803685c464d6197b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>The study investigates the efficacy of a novel drug, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aliprex\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", in treating patients with Alzheimer's disease. <br>The research was conducted at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the University of California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UCSF\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") and involved \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " participants. <br>Preliminary results indicate a significant improvement in cognitive function among patients receiving \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aliprex\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " compared to the placebo group. <br>The research team, led by Dr. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Emily Carter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", plans to publish the full findings in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the New England Journal of Medicine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".<br></div></span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Aliprex, Label: ORG\n",
      "Entity: the University of California, Label: ORG\n",
      "Entity: San Francisco, Label: GPE\n",
      "Entity: UCSF, Label: ORG\n",
      "Entity: 100, Label: CARDINAL\n",
      "Entity: Aliprex, Label: ORG\n",
      "Entity: Emily Carter, Label: PERSON\n",
      "Entity: the New England Journal of Medicine, Label: ORG\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T17:54:23.931683Z",
     "start_time": "2024-12-30T17:54:23.614783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = \"dslim/bert-base-NER\"  # Replace with the actual path\n",
    "tokenizer_path = \"dslim/bert-base-NER\" # Replace with the actual path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "text = \"Apple Inc. plans to open a new store in San Francisco by January 2024. Tim Cook, the CEO, announced the news yesterday.\"\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "for entity in ner_results:\n",
    "    print(entity)"
   ],
   "id": "8d4594b110674be3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-ORG', 'score': 0.9996086, 'index': 1, 'word': 'Apple', 'start': 0, 'end': 5}\n",
      "{'entity': 'I-ORG', 'score': 0.99942136, 'index': 2, 'word': 'Inc', 'start': 6, 'end': 9}\n",
      "{'entity': 'B-LOC', 'score': 0.99934715, 'index': 11, 'word': 'San', 'start': 40, 'end': 43}\n",
      "{'entity': 'I-LOC', 'score': 0.99942625, 'index': 12, 'word': 'Francisco', 'start': 44, 'end': 53}\n",
      "{'entity': 'B-PER', 'score': 0.9997869, 'index': 18, 'word': 'Tim', 'start': 71, 'end': 74}\n",
      "{'entity': 'I-PER', 'score': 0.99977297, 'index': 19, 'word': 'Cook', 'start': 75, 'end': 79}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-30T17:54:53.315962Z",
     "start_time": "2024-12-30T17:54:53.044820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline as hf_pipeline  # To avoid naming conflict\n",
    "\n",
    "# Specify the pre-trained model\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"Angela Merkel visited the White House in Washington, D.C. on Tuesday.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "\n",
    "import torch\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Map token ids to label names\n",
    "id2label = model.config.id2label\n",
    "predicted_labels = [id2label[prediction.item()] for prediction in predictions[0]]\n",
    "\n",
    "# Align tokens and labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Combine sub-word tokens and their labels to form entities\n",
    "entities = []\n",
    "current_entity = None\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    if token.startswith(\"##\"):  # Handle sub-word tokens\n",
    "        if current_entity:\n",
    "            current_entity[\"word\"] += token[2:]\n",
    "    elif label.startswith(\"B-\"):\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "        current_entity = {\"entity\": label[2:], \"word\": token}\n",
    "    elif label.startswith(\"I-\"):\n",
    "        if current_entity and label[2:] == current_entity[\"entity\"]:\n",
    "            current_entity[\"word\"] += \" \" + token\n",
    "        else:\n",
    "            # Handle cases where I- tag follows a different or no B- tag\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"entity\": label[2:], \"word\": token}\n",
    "    else:  # \"O\" label (outside of any entity)\n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "if current_entity:\n",
    "    entities.append(current_entity)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity)"
   ],
   "id": "31e8578392473d32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'PER', 'word': 'Angela Merkel'}\n",
      "{'entity': 'LOC', 'word': 'White House'}\n",
      "{'entity': 'LOC', 'word': 'Washington , D . C .'}\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
