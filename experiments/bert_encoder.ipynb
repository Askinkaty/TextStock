{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f152b0b-bf26-4884-b076-2d019905e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text=text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt')\n",
    "            \n",
    "        return {'input_ids': encoding['input_ids'].squeeze(0),\n",
    "               'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "               'label': torch.tensor(label, dtype=torch.long)}\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e587462-9a42-448c-be37-7af0f72bad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and split\n",
    "data_dir = '/projappl/project_2006600/fin_experiment/data'\n",
    "data_combined_news = pd.read_csv(os.path.join(data_dir, 'data_combined_news.csv'), sep='\\t', encoding='utf-8')\n",
    "clean_news = data_combined_news['All_news_clean']\n",
    "\n",
    "x = data_combined_news['All_news_clean']\n",
    "y = data_combined_news['Label']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a644e1-7786-4e99-94de-d6ebfa18c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb86770-cb12-44b5-8c89-160b5428a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsDataset(X_train.values, y_train.values, tokenizer, 500)\n",
    "valid_dataset = NewsDataset(X_valid.values, y_valid.values, tokenizer, 500)\n",
    "#next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8465e5a6-b0a2-4d4d-ba42-120c592a7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=12, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0678489a-d6ba-44ea-ba23-0565dca746c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class BertForNewsClassification(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, args):\n",
    "        super(BertForNewsClassification, self).__init__()\n",
    "        self.args = args\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(self.args.dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, self.args.num_classes)\n",
    "        # Freeze BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78885029-57fb-4c6b-afda-ee48e0360966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass \n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    num_classes: int\n",
    "    dropout: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "652370b3-6c0e-4efd-9ebe-60b493bbbf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A100-SXM4-40GB MIG 1g.5gb\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "args = TrainArgs(2e-5, 16, 10, 2, 0.3)\n",
    "\n",
    "model = BertForNewsClassification(\"bert-base-uncased\", args)"
   ],
   "id": "35875edb282b4180"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd3ec86-60bd-4c84-b6fd-3c843c290a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39993e66-39c0-432c-b6ed-6e65c1d53282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs):\n",
    "    model.to(device)\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = evaluate_model(model, val_loader, loss_fn, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "                        \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            predictions = torch.argmax(probs, dim=1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Validation accuracy: {acc}')\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589bd961-1e92-4b98-ba86-918a9dfae926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.7184\n",
      "Validation accuracy: 0.4949748743718593\n",
      "Validation Loss: 0.6980\n",
      "Saved best model.\n",
      "Epoch 2/10\n",
      "Train Loss: 0.7090\n",
      "Validation accuracy: 0.535175879396985\n",
      "Validation Loss: 0.6955\n",
      "Saved best model.\n",
      "Epoch 3/10\n",
      "Train Loss: 0.7072\n",
      "Validation accuracy: 0.4824120603015075\n",
      "Validation Loss: 0.6949\n",
      "Saved best model.\n",
      "Epoch 4/10\n",
      "Train Loss: 0.7037\n",
      "Validation accuracy: 0.542713567839196\n",
      "Validation Loss: 0.6905\n",
      "Saved best model.\n",
      "Epoch 5/10\n",
      "Train Loss: 0.7067\n",
      "Validation accuracy: 0.5603015075376885\n",
      "Validation Loss: 0.6931\n",
      "Epoch 6/10\n",
      "Train Loss: 0.6944\n",
      "Validation accuracy: 0.5678391959798995\n",
      "Validation Loss: 0.6885\n",
      "Saved best model.\n",
      "Epoch 7/10\n",
      "Train Loss: 0.6984\n",
      "Validation accuracy: 0.5577889447236181\n",
      "Validation Loss: 0.6897\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, optimizer, criterion, device, args.epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
